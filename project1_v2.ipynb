{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "DATA_TRAIN_PATH = \"../data/train.csv\"\n",
    "y, tX, ids = load_csv_data(DATA_TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(250000,)\n",
      "(250000, 30)\n",
      "(250000,)\n",
      "[-9.99000e+02  8.51860e+01  6.88270e+01  5.04200e+00 -9.99000e+02\n",
      " -9.99000e+02 -9.99000e+02  2.11600e+00  5.04200e+00  7.14430e+01\n",
      "  1.55800e+00 -1.35100e+00 -9.99000e+02  2.79310e+01  1.17500e+00\n",
      "  2.35600e+00  4.35120e+01  2.33200e+00  5.84000e-01  4.46980e+01\n",
      " -2.03300e+00  1.51816e+02  0.00000e+00 -9.99000e+02 -9.99000e+02\n",
      " -9.99000e+02 -9.99000e+02 -9.99000e+02 -9.99000e+02  0.00000e+00]\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(y))\n",
    "print(np.shape(tX))\n",
    "print(np.shape(ids))\n",
    "print(tX[18,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do your thing crazy machine learning thing here :) ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def  replace_data_point_by_mean(x):# replace missing datapoint of matrix x by the mean of each column\n",
    "    \n",
    "    col = x.shape[1]\n",
    "    row = x.shape[0]\n",
    "    \n",
    "    for i in range(col):\n",
    "        if -999 in x[:,i]:\n",
    "            ind_nan = []\n",
    "            compressed =[]\n",
    "            for j in range (row):\n",
    "                if x[j,i] == -999:\n",
    "                    ind_nan.append(j)\n",
    "                else:\n",
    "                    compressed.append(x[j,i])\n",
    "            mean = np.mean(compressed)\n",
    "            \n",
    "            x[:,i] = np.where(x[:,i] == -999, mean, x[:,i])\n",
    "            \n",
    "    return x\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from implementations import *\n",
    "from plots import *\n",
    "\n",
    "''' bias variance + cross validation + ridge regression ->  get optimal degree and lambda for each parameter\n",
    "'''\n",
    "\n",
    "def bias_variance_demo_with_cross_validation_and_ridge(y,x): # param x,y\n",
    "    \"\"\"The entry.\"\"\"\n",
    "    # define parameters\n",
    "    seed = 1 #range(100)\n",
    "    num_data = 10000\n",
    "    ratio_train = 0.005\n",
    "    degrees = range(1, 10)\n",
    "    lambdas = np.logspace(-4, 0, 30)\n",
    "    k_fold = 4\n",
    "    \n",
    "    \n",
    "    # define list to store the variable\n",
    "    rmse_tr = np.empty(len(degrees))\n",
    "    rmse_te = np.empty(len(degrees))\n",
    "    \n",
    "    #for index_seed, seed in enumerate(seeds):\n",
    "    #    np.random.seed(seed)\n",
    "        #x = np.linspace(0.1, 2 * np.pi, num_data) # to delete\n",
    "        #y = np.sin(x) + 0.3 * np.random.randn(num_data).T # to delete\n",
    "        # ***************************************************\n",
    "        # INSERT YOUR CODE HERE\n",
    "        # split data with a specific seed:\n",
    "    x_tr, x_te, y_tr, y_te = split_data(x, y, ratio_train, seed)\n",
    "    k_indices = build_k_indices(y_tr, k_fold, seed)\n",
    "        # ***************************************************\n",
    "        # ***************************************************\n",
    "        # INSERT YOUR CODE HERE\n",
    "        # bias_variance_decomposition: TODO\n",
    "    for index_degree, degree in enumerate(degrees):\n",
    "        tx_tr = build_poly(x_tr,degree)\n",
    "        tx_te = build_poly(x_te,degree)\n",
    "        #weights = least_squares(y_tr,tx_tr)\n",
    "            \n",
    "        # mean loss over all folds\n",
    "        loss_tr = []\n",
    "        loss_te = []\n",
    "            \n",
    "        for ind, lambda_ in enumerate(lambdas):\n",
    "            # losses of each fold\n",
    "            losses_tr = [] \n",
    "            losses_te = []\n",
    "                \n",
    "            for k in range(k_fold):\n",
    "                l_tr, l_te = cross_validation(y, x, k_indices, k, lambda_, degree)\n",
    "                losses_tr.append(l_tr)\n",
    "                losses_te.append(l_te)\n",
    "            loss_tr.append(np.mean(losses_tr))\n",
    "            loss_te.append(np.mean(losses_te))\n",
    "        # Find optimal lambda\n",
    "        min_loss_te = min(loss_te)\n",
    "        indices = [i for i, v in enumerate(loss_te) if v == min_loss_te] # find indices with minimum loss\n",
    "        lambda_te = lambdas[indices[-1]] # get largest lambda for which loss is minimum\n",
    "            \n",
    "        weights = ridge_regression(y_tr,tx_tr,lambda_te) # use optimal lambda to ridge and find weigths\n",
    "            \n",
    "        rmse_tr[index_degree] = np.sqrt(2*compute_loss(y_tr,tx_tr,weights))\n",
    "        rmse_te[index_degree] = np.sqrt(2*compute_loss(y_te,tx_te,weights))\n",
    "            # ***************************************************\n",
    "    \n",
    "    # bias_variance_decomposition_visualization(degrees, rmse_tr, rmse_te)\n",
    "\n",
    "    \n",
    "    # Find optimal degree for polynomial regression\n",
    "    rmse_te_mean = np.expand_dims(np.mean(rmse_te, axis=0), axis=0) # rmse mean of each row, for each degree\n",
    "    ind_min_degree = np.unravel_index(np.argmin(rmse_te_mean, axis=None), rmse_te_mean.shape)\n",
    "    degree_te = degrees[ind_min_degree[-1]]\n",
    "    \n",
    "    return degree_te, lambda_te\n",
    "    \n",
    "# bias_variance_demo_with_cross_validation_and_ridge() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate predictions and save ouput in csv format for submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = \"../data/test.csv\"\n",
    "_, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(568238, 30)\n",
      "process finished\n",
      "Bias Variance & Cross Validation & Ridge (0):\n",
      "Bias Variance & Cross Validation & Ridge (1):\n",
      "Bias Variance & Cross Validation & Ridge (2):\n",
      "Bias Variance & Cross Validation & Ridge (3):\n",
      "Bias Variance & Cross Validation & Ridge (4):\n",
      "Bias Variance & Cross Validation & Ridge (5):\n",
      "Bias Variance & Cross Validation & Ridge (6):\n",
      "Bias Variance & Cross Validation & Ridge (7):\n",
      "Bias Variance & Cross Validation & Ridge (8):\n",
      "Bias Variance & Cross Validation & Ridge (9):\n",
      "Bias Variance & Cross Validation & Ridge (10):\n",
      "Bias Variance & Cross Validation & Ridge (11):\n",
      "Bias Variance & Cross Validation & Ridge (12):\n",
      "Bias Variance & Cross Validation & Ridge (13):\n",
      "Bias Variance & Cross Validation & Ridge (14):\n",
      "Bias Variance & Cross Validation & Ridge (15):\n",
      "Bias Variance & Cross Validation & Ridge (16):\n",
      "Bias Variance & Cross Validation & Ridge (17):\n",
      "Bias Variance & Cross Validation & Ridge (18):\n",
      "Bias Variance & Cross Validation & Ridge (19):\n",
      "Bias Variance & Cross Validation & Ridge (20):\n",
      "Bias Variance & Cross Validation & Ridge (21):\n",
      "Bias Variance & Cross Validation & Ridge (22):\n",
      "Bias Variance & Cross Validation & Ridge (23):\n",
      "Bias Variance & Cross Validation & Ridge (24):\n",
      "Bias Variance & Cross Validation & Ridge (25):\n",
      "Bias Variance & Cross Validation & Ridge (26):\n",
      "Bias Variance & Cross Validation & Ridge (27):\n",
      "Bias Variance & Cross Validation & Ridge (28):\n",
      "Bias Variance & Cross Validation & Ridge (29):\n"
     ]
    }
   ],
   "source": [
    "def demo():\n",
    "    \n",
    "    # Pre-processing of data -> delete nan and replace by mean column value\n",
    "    processed_tX_train = replace_data_point_by_mean(tX)\n",
    "    processed_tX_test = replace_data_point_by_mean(tX_test)\n",
    "    print(np.shape(processed_tX_test))\n",
    "    print('process finished')\n",
    "    \n",
    "    final_weights = np.array([]) # weights of all parameters with polynomial\n",
    "    final_tX_test = np.array([]) # final dataset with ridge\n",
    "    \n",
    "    # find optimal lambda and degree for each parameters\n",
    "    for i in range(processed_tX_train.shape[1]): \n",
    "        \n",
    "        degree_te, lambda_te = bias_variance_demo_with_cross_validation_and_ridge(y,processed_tX_train[:,i])\n",
    "        all_tx_tr = build_poly(processed_tX_train[:,i],degree_te)\n",
    "        \n",
    "        #1e itération\n",
    "        if final_weights.size == 0:\n",
    "            final_weights = ridge_regression(y,all_tx_tr,lambda_te)\n",
    "            final_tX_test = build_poly(processed_tX_test[:,i],degree_te)\n",
    "        \n",
    "        # toutes les itérations qui suivent\n",
    "        else:\n",
    "            final_weights = np.concatenate((final_weights,np.array(ridge_regression(y,all_tx_tr,lambda_te))))\n",
    "            final_tX_test = np.concatenate((final_tX_test,np.array(build_poly(processed_tX_test[:,i],degree_te))), axis=1)\n",
    "        \n",
    "        print(\"Bias Variance & Cross Validation & Ridge ({bi}):\".format(\n",
    "              bi=i))\n",
    "        \"\"\"\n",
    "        print('Degree: ')\n",
    "        print(degree_te)\n",
    "        print('lambda: ')\n",
    "        print(lambda_te)\n",
    "        \n",
    "        print(np.shape(final_weights))\n",
    "        print(final_weights)\n",
    "        print(np.shape(final_tX_test))\n",
    "        print(final_tX_test)\n",
    "        \"\"\"\n",
    "        \n",
    "    return final_weights, final_tX_test\n",
    "\n",
    "final_weights, final_tX_test = demo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60,)\n",
      "(568238, 60)\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(final_weights))\n",
    "print(np.shape(final_tX_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_PATH = \"../data/submission002.csv\"\n",
    "#y_pred = predict_labels(weights, tX_test)\n",
    "y_pred = predict_labels(final_weights, final_tX_test)\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
