{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "from implementations import *\n",
    "from helpers import *\n",
    "from data_manager import *\n",
    "from test_runners import *\n",
    "from plot import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test consol"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Test consol allows to simulate all learning methods with any parameters (presented in forms of lists), any processing data tool and by spliting or cross validating the data\n",
    "\n",
    "Reminder for My_options (processing data):\n",
    "     nandel -> delete nan values (-999.)\n",
    "     nanmed -> replace -999. with mean\n",
    "     bound -> eliminate outliers\n",
    "     std -> standardize data set\n",
    "     prb -> change y into probability values\n",
    "     zerovar -> eliminate columns with no varinaces\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "runing degree 1\n",
      "runing degree 2\n",
      "runing degree 3\n",
      "runing degree 4\n",
      "runing degree 5\n",
      "runing degree 6\n",
      "Score: % 75.128\n",
      "Score: % 80.096\n",
      "Score: % 80.624\n",
      "Score: % 81.716\n",
      "Score: % 81.792\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Nino54/Desktop/EPFL/Master/project1/ML_cortana/scripts_propre/plot.py:114: MatplotlibDeprecationWarning: \n",
      "The frameon kwarg was deprecated in Matplotlib 3.1 and will be removed in 3.3. Use facecolor instead.\n",
      "  frameon=None, metadata=None)\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: % 81.964\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAgAElEQVR4nO3deXwV9b3/8dcnG4GwE40LS6Ki4AIiiFulIGptb+t2qQWtlVbFey1cl9qqrVVq6+1qrbfa3qvWWv0pKeJuqeIWrVYqIqACgsgaENkxCYRsn98fM8FDmJBjkuGQnPfz8TiPc2bm+53z+Uacz5nvd+Y75u6IiIg0lJHqAEREZN+kBCEiIpGUIEREJJIShIiIRFKCEBGRSEoQIiISSQlCpA0zs/Fm9nqq45D2SQlC9glmVmJmm82sQ6pjEZGAEoSknJkVAqcCDpy9l787a29+X1uhv4uAEoTsG74FzAQeAC5J3GBmHc3sdjNbYWZbzex1M+sYbvuCmf3TzLaY2SozGx+uLzGzyxL2sUs3jJm5mX3XzD4EPgzX3Rnu41Mzm21mpyaUzzSzH5rZR2ZWFm7vY2Z3m9ntDeJ9xsyubthAM/tfM/tNg3VPmdm14efrzWx1uP9FZjY66g9lZr3M7OkwzreAQxtsH2BmL5jZpnA/FzSo+0xYd5aZ/SyJv8ue9tfBzH5jZivN7JOwjR2j4pY2yt310iulL2AJcCUwFKgGChK23Q2UAAcDmcDJQAegL1AGjAOygV7AsWGdEuCyhH2MB15PWHbgBaAn0DFc981wH1nA94C1QG647fvAe8ARgAGDw7LDgTVARlguH9iWGH/Cd44AVgEWLvcAtgMHhftdBRwUbisEDm3kb1UMTAXygKOB1fVtC9etAr4dtuM4YANwVELdYqATcGRYttG/SxL7+x3wdFi+C/AM8PNU/3vSqxX/30x1AHql9wv4QpgU8sPlD4Brws8Z4UF0cES9G4EnGtlnMgnitCbi2lz/vcAi4JxGyi0Ezgg/TwSmN1LOgJXAiHD5cuDl8PNhwDrgdCB7DzFlhn+rAQnr/jshQXwD+EeDOv8H3JJQ94iEbT/b09+lif0ZUJGYyICTgGWp/jelV+u91MUkqXYJMMPdN4TLj/BZN1M+kAt8FFGvTyPrk7UqccHMvmdmC8NurC1At/D7m/quvxCcfRC+PxRVyIMjaDHBGQ/AhcDD4bYlwNXAZGCdmRWb2UERu9mP4Jd8YuwrEj73A04Iu9y2hO24CDigkbq7/A0i1jW1v07A7IRtz4XrpZ3QQJSkTNhffQGQaWZrw9UdgO5mNpigW6eSoJ99XoPqqwi6eKJUEBy86h0QUWbnNMbheMP1wGhgvrvXmdlmgl/J9d91KPB+xH7+H/B+GO9A4MlGYgKYAswws18AJwDn7QzG/RHgETPrSvAr/ZfAxQ3qrwdqCBLWB+G6vgnbVwGvuvsZDb/YzDLDur2BxeHqPhExJk7vvKf91Z/dHeXuqyNbK22eziAklc4Fagn6w48NXwOBfwDfcvc64H7gt2Z2UDhYfFJ4KezDwOlmdoGZZYUDsMeG+50LnG9mnczsMODSJuLoQnDwXA9kmdnNQNeE7fcBPzWz/hYYZGa9ANy9FJhFcObwmLtvb+xL3H1O+B33Ac+7+xYAMzvCzE4L21VJcOCtjahfCzwOTA7bdiS7Duo/CxxuZhebWXb4Ot7MBkbUHUBwccCe7Gl/dcC9wB1mtn/YjoPN7EtN7FPaECUISaVLgD+7+0p3X1v/Au4CLrLgUsvrCM4kZgGbCH5ZZ7j7SuArBAPKmwiSwuBwv3cAVcAnBF1ADzcRx/PA3wl+Wa8gOEgndrX8lmBgeAbwKfAngkHcen8BjqGR7qUGphCMNTySsK4D8AuCAeC1wP7ADxupPxHoHJZ7APhz/QZ3LwPOBMYSDJ6vJfh7dUio2y1c/1AYy47GAk1if9cTXGAw08w+BV4kGHCXdqL+igoRaSYzG0HQ1VQY/rJuE8zsl8AB7n5Jk4UlLekMQqQFzCwbuAq4b19PDuE9DYPCbrLhBF1vT6Q6Ltl3KUGINJOZDQS2AAcS3BOwr+tCMA5RQdBldjvwVEojkn2auphERCSSziBERCRSu7kPIj8/3wsLC5tdv6Kigry8vNYLqA1ItzanW3tBbU4XLWnz7NmzN7h75A2O7SZBFBYW8vbbbze7fklJCSNHjmy9gNqAdGtzurUX1OZ00ZI2m9mKxrapi0lERCIpQYiISCQlCBERidRuxiCiVFdXU1paSmVlZZNlu3XrxsKFC/dCVHtHbm4uvXv3Jjs7O9WhiEgb1a4TRGlpKV26dKGwsBAz22PZsrIyunTpspcii5e7s3HjRkpLSykqKkp1OCLSRrXrLqbKykp69erVZHJob8yMXr16JXXmJCLSmHadIIC0Sw710rXdItJ62n2CEBGR5lGCEBGRSEoQ+6ja2t0eKCYislcpQTQwe8Vm7n5lCbNXbG61fT744IMMGjSIwYMHc/HFFzN+/HimTZu2c3vnzp2B4Hb5UaNGceGFF3LMMcdw/fXX84c//GFnucmTJ3P77bcD8Otf/5rjjz+eQYMGccstt7RarCIi9dr1Za6JfvLMfBas+bTR7bW1tWyrruODtWXUOWQYDDigC11yG7+P4MiDunLL147a4/fOnz+f2267jTfeeIP8/Hw2bdrEtdde22j5t956i/fff5+ioiLmzJnD1VdfzZVXXgnA1KlTee6555gxYwYffvghb731Fu7O2WefzWuvvcaIESOa+CuIiCQv1jMIMzvLzBaZ2RIzuyFie18ze8XM5pjZu2b2lYjt5WZ2XZxx1vu0soa68PEYdR4st9TLL7/MmDFjyM/PB6Bnz557LD98+PCd9y4MGTKEdevWsWbNGubNm0ePHj3o27cvM2bMYMaMGQwZMoTjjjuODz74gA8//LDFsYqIJIrtDMLMMoG7gTOAUmCWmT3t7gsSit0ETHX3P5rZkcB0oDBh+x0ED5NvsaZ+6ZeVlbF4Uw0X3TeT6po6srMyuHPsEIb269Gi73X33S45zcrKoq6ubuf2qqqqndsaTtk7ZswYpk2bxtq1axk7duzOOjfeeCNXXHFFi2ITEdmTOM8ghgNL3H2pu1cBxcA5Dco40DX83A1YU7/BzM4FlgLzY4xxF0P79eDhy07k2jOP4OHLTmxxcgAYPXo0U6dOZePGjQBs2rSJwsJCZs+eDcBTTz1FdXV1o/XHjh1LcXEx06ZNY8yYMQB86Utf4v7776e8vByA1atXs27duhbHKiKSKM4xiIOBVQnLpcAJDcpMBmaY2SQgDzgdwMzygOsJzj4a7V4yswnABICCggJKSkp22d6tWzfKysqSCra2tpaysjIO75nF4T0LAJKuuyd9+/bl2muv5dRTTyUzM5NBgwZx6623MnbsWIYOHcrIkSPJy8ujrKyMbdu2UVNTs8v39u3bl61bt3LAAQfQuXNnysrKOOmkkzj//PM54YTgz5mXl8e9995Lx44dd/nuysrK3f4micrLy/e4vb1Jt/aC2pwuYmuzu8fyAr4O3JewfDHw+wZlrgW+F34+CVhAcFbzG+CCcP1k4Lqmvm/o0KHe0IIFC3Zb15hPP/006bJtRVPtf+WVV/ZOIPuIdGuvu9qcLlrSZuBtb+S4GucZRCnQJ2G5NwldSKFLgbMA3P1NM8sF8gnONMaY2a+A7kCdmVW6+10xxisiIgniTBCzgP5mVgSsBsYCFzYosxIYDTxgZgOBXGC9u59aX8DMJgPlSg4iIntXbIPU7l4DTASeBxYSXK0038xuNbOzw2LfAy43s3nAFGB8eMrTmnG05u7ajHRtt4i0nlhvlHP36QSXriauuznh8wLglCb2Mbm535+bm8vGjRvTbspvD58HkZubm+pQRKQNa9d3Uvfu3ZvS0lLWr1/fZNnKysp2dUCtf6KciEhztesEkZ2dnfQT1UpKShgyZEjMEYmItB2arE9ERCIpQYiISCQlCBERiaQEISIikZQgREQkkhKEiIhEUoIQEZFIShAiIhJJCUJERCIpQYiISCQlCBERiaQEISIikZQgREQkkhKEiIhEUoIQEZFIShAiIhJJCUJERCIpQYiISCQlCBERiaQEISIikZQgREQkkhKEiIhEijVBmNlZZrbIzJaY2Q0R2/ua2StmNsfM3jWzr4TrzzCz2Wb2Xvh+WpxxiojI7rLi2rGZZQJ3A2cApcAsM3va3RckFLsJmOrufzSzI4HpQCGwAfiau68xs6OB54GD44pVRER2F+cZxHBgibsvdfcqoBg4p0EZB7qGn7sBawDcfY67rwnXzwdyzaxDjLGKiLRJs1ds5tmPqpi9YnOr7zu2MwiCX/yrEpZLgRMalJkMzDCzSUAecHrEfv4dmOPuO+IIUkRkX1JdW8e2HbVUVNVQsaOGiqpattW/V9VQseOz96Ubynn23Y+prXOeXT6Thy87kaH9erRaLHEmCItY5w2WxwEPuPvtZnYS8JCZHe3udQBmdhTwS+DMyC8wmwBMACgoKKCkpKTZwZaXl7eofluUbm1Ot/aC2hwnd6fGYUcNVNb6Z++1sL0meK/c5d2prIUdNcH7zm2JdWugpuFRcg8yDOrC8lXVdUx5cRZlh+a0WhvjTBClQJ+E5d6EXUgJLgXOAnD3N80sF8gH1plZb+AJ4Fvu/lHUF7j7PcA9AMOGDfORI0c2O9iSkhJaUr8tSrc2p1t7If3aPHvFZp59cRbjjhm8yy9pd2dHTR0VO2rYVlX/67zhL/I9/1rfVlVDeX398L2mLvmjeaecTDrlZJHXIXzvmMl+HbLIa7C+c4cG5erfc7Lo1CFz53un7EzmlW7lovtmUlVdR052BuNOP77NnEHMAvqbWRGwGhgLXNigzEpgNPCAmQ0EcoH1ZtYd+Btwo7u/EWOMIu1Wfd90l6LNrXrQiFJb51TX1lFdW0dNrVNdV0d1rVNTG7zvsr6mjpqd5cMydR6ujyrvDdYnlv/sezeWV/HOys3UOTy25J8c2C2XujqoqAoO5rVJHszNCA7COZnkdQjfc7LokZdD7x6ddq7/7MCdSacODQ7g9WXCbZ2yM8nIiOpUaZmh/Xrw8GUnMuXFWa2eHCDGBOHuNWY2keAKpEzgfnefb2a3Am+7+9PA94B7zewagu6n8e7uYb3DgB+b2Y/DXZ7p7uviilekPfnX0o1c/Ke3qKqt48mP3uTcIQeR3zk3PGAHB9jdDt67rI84UO8s1+BAXVuHf45ukebIMMjKzCAnM4OsTCMrI4OcTCMrXM7JzGDztqqd3S3u0DU3m2P7dG/k13j9Qf2z9XnhQT43OwOz1j+Yx2Vovx6UHZoTy4+AOM8gcPfpBJeuJq67OeHzAuCUiHo/A34WZ2wi7c3miipeWbSOlxau44UFn1BVWwdATZ0zbfZqcrIyyM4IDqrZmRlkZxpZmRZ8zsggOys48GaH6zrmBOWzw4PwZ3US139WPivTgv3Ul8n8rExOeFCvP5gnHtg/O+CHnxPLhN+TmcSv79krNu/S3XLbecfEfubU3sWaIEQkPu7OR+vLeXHhOl5a+AmzVwTdK/t16cCIw/N5dfF6amudnOyMVr+6ZV8Ud3dLOlKCEGlDqmrqmLV8Ey8u/ISXFq5j5aZtABx1UFcmntaf0wfuz9EHdSMjw5i9YnPaHSzj7G5JR0oQIvu4xK6j1xavp2xHDTlZGZxyaC8mjDiE0QP358BuHXerp4OltJQShMg+Zk9dR/826EBGDyzglMN60SlH//tKvPQvTGQfsMeuo1GHMXpgAccc3C2WSyVFGqMEIZIimyuqKFm8jhcXruO1Rcl3HYnsLUoQIntJU11Hpw3Yny/0z1fXkewz9C9RJEbVtXW8tWwTLy1cx0sffMKKjUHX0ZEHqutI9n1KECKtbE9dR5efeginDdifg7qr60j2fUoQIi3UWNdRfucOfOWYAxk9UF1H0jbpX6xIM6jrSNKBEoRIkvbUdXTZqYcwWl1H0s4oQYg0IrHr6OWF63h7xSZ1HUla0b9skQTVtXXMWrYpGE+I6Do6bWABg9R1JGlCCULSXmNdRyer60jSnBKEpJ36rqOXFgYT4KnrSCSa/g+QtFCyaB33v7eDaavf4b01W3d2HQ08sCvfDa86UteRyK6UIKTd+LSymuUbKlgWvpZvqGDZxm0s+aSMiqraoNDqjxnSt7u6jkSSoAQhbUrFjhqWb6xg+YZtLNtQzrIN28LlCjZWVO0sZwYHdetIYX4nDtu/M++WbsWBTIPTBxZw8Yn9UtcIkTZCCUL2OZXVtazYuK3BmUDwvq5sxy5lC7p2oLBXHmccWUBhfh6FvfI4ZL88+vbsRG52JrDrs4qzszI48ZBeqWiWSJujBCEpUVVTx8pN2z7rEgoTwPINFazZWrlL2fzOORT2ymPE4ftRlJ9HUZgICvM7JTWQrGcVizSPEoTEpqa2jtLN23ce/HeeEWysYPXm7dT5Z2W7d8qmsFceJxzSi8JeeRTtl0dRrzz65Xeia252i2PR4zdFPj8lCGmR2jpnzZbtLN+46+Dw8o3bWLVpGzUJWaBLhywK8/M4tk8Pzjv2YIr2C84EivLz6N4pJ4WtEJEoShDSpLo655Oyyl3HBMLB4ZUbt1FVW7ezbMfsTArz8xh4YBe+fPQBn3UJ5efRKy8HM11GKtJWKEGkqdkrNvPsR1V0KdrM0H49cHfWl+9g+YZgXGDpzjOB4FVZ/VkSyMnKoLBXJw7Jz2P0gP13JoCi/Dz279JBSUCknYg1QZjZWcCdQCZwn7v/osH2vsBfgO5hmRvcfXq47UbgUqAW+C93fz7OWNPJ28s3Me7emVTXOo8v+Sf9enVifVkV5TtqdpbJzjT69OxEUa88vnBY/s4EUJifx4Fdc3VDmUgaiC1BmFkmcDdwBlAKzDKzp919QUKxm4Cp7v5HMzsSmA4Uhp/HAkcBBwEvmtnh7l4bV7zp5M6XPqS6NhgbCIYIjDFDe392JtArj4O655KVmZHSOEUkteI8gxgOLHH3pQBmVgycAyQmCAe6hp+7AWvCz+cAxe6+A1hmZkvC/b0ZY7xpYfaKTbyxZAMZBjjkZGfwm68P1tU9IrKbOBPEwcCqhOVS4IQGZSYDM8xsEpAHnJ5Qd2aDugc3/AIzmwBMACgoKKCkpKTZwZaXl7eofltQUe3c/MZ2euUalxyVw6L1lQw+IIeyZfMoWZbq6OKXDv+NG1Kb00NcbY4zQUR1UnuD5XHAA+5+u5mdBDxkZkcnWRd3vwe4B2DYsGE+cuTIZgdbUlJCS+rv69ydKx9+h61V25n2nydzbJ/u7b7NDaVbe0FtThdxtTnOBFEK9ElY7s1nXUj1LgXOAnD3N80sF8hPsq58Dg//ayV/f38tN355AMf26Z7qcESkDYhzFHIW0N/Miswsh2DQ+ekGZVYCowHMbCCQC6wPy401sw5mVgT0B96KMdZ27YO1n/LTZxcw4vD9uPzUQ1Idjoi0EbGdQbh7jZlNBJ4nuIT1fnefb2a3Am+7+9PA94B7zewagi6k8e7uwHwzm0owoF0DfFdXMDXP9qpaJj4yhy652dz+9cG6PFVEkpZUgjCzx4D7gb+7e11T5euF9zRMb7Du5oTPC4BTGql7G3Bbst8l0W59dj4frS/noe+cwH5dOqQ6HBFpQ5LtYvojcCHwoZn9wswGxBiTtJJn5q1hylur+I8vHsoX+uenOhwRaWOSShDu/qK7XwQcBywHXjCzf5rZt82s5VNtSqtbtWkbP3z8PYb07c61Zxye6nBEpA1KepDazHoB44HLgDkEU2gcB7wQS2TSbNW1dUyaMgcM/mfsELJ1R7SINEOyYxCPAwOAh4CvufvH4aa/mtnbcQUnzXP7jMXMXbWFP1x0HH16dkp1OCLSRiV7FdNd7v5y1AZ3H9aK8UgLvbZ4Pf/76kdceEJfvnLMgakOR0TasGT7Hgaa2c67q8ysh5ldGVNM0kzryiq5dupcDi/ozM1fPTLV4YhIG5dsgrjc3bfUL7j7ZuDyeEKS5qirc743dR7lO2q468LjyM3OTHVIItLGJZsgMizhKTDhVN56RuQ+5P9eW8o/PtzAzV89isMLuqQ6HBFpB5Idg3gemGpm/0twx/N/AM/FFpV8Lu+s3MztMxbxb8ccyLjhfZquICKShGQTxPXAFcB/Esy0OgO4L66gJHlbt1fzX1PmUNA1l/8+/xg97lNEWk1SCSKcXuOP4Uv2Ee7ODx9/j4+3VvLof5xEt466Z1FEWk+y90H0B34OHEkw4yoA7q6pQVOoeNYq/vbex1x/1gCO66snwolI60p2kPrPBGcPNcAo4EGCm+YkRRZ/UsZPnpnPqf3zuWKE8rSItL5kE0RHd38JMHdf4e6TgdPiC0v2pLK6lomPvEPnDlncfoGm8BaReCQ7SF1pZhkEs7lOBFYD+8cXluzJT59dwOJPynnwO8PZv0tu0xVERJoh2TOIq4FOwH8BQ4FvApfEFZQ07u/vfczD/1rJFV88hBGH75fqcESkHWvyDCK8Ke4Cd/8+UA58O/aoJNKqTdv4wWPvMrhPd64784hUhyMi7VyTZxDhoz6Hmi6wT6nq2jquKp4DDr/XFN4ishckOwYxB3jKzB4FKupXuvvjsUQlu7njhcW8s3ILvx83hL69NIW3iMQv2QTRE9jIrlcuOaAEsRe8/uEG/vjqR4w9vg9fG3xQqsMRkTSR7J3UGndIkQ3lO7hm6lwO3a8zt3ztqFSHIyJpJNk7qf9McMawC3f/TqtHJDvVT+G9dXs1D106nI45msJbRPaeZLuYnk34nAucB6xp/XAk0X2vL+XVxev52blHM+CArqkOR0TSTLJdTI8lLpvZFODFWCISAOat2sKvnlvEl48+gItO6JvqcEQkDTX3Wsn+QJNHLTM7y8wWmdkSM7shYvsdZjY3fC02sy0J235lZvPNbKGZ/U86XWZbVlnNpHAK71+cP0hTeItISiQ7BlHGrmMQawmeEbGnOpnA3cAZQCkwy8yedvcF9WXc/ZqE8pOAIeHnk4FTgEHh5teBLwIlycTblrk7P3zifVZv2c7UK06kWydN4S0iqZFsF1NznmE5HFji7ksBzKwYOAdY0Ej5ccAt9V9JMNaRQ/CAomzgk2bE0OY8+nYpz8xbw/e/dARD+/VMdTgiksbMfbeLk3YvZHYe8LK7bw2XuwMj3f3JPdQZA5zl7peFyxcDJ7j7xIiy/YCZQO/wzm3M7DfAZQQJ4i53/1FEvQnABICCgoKhxcXFTbalMeXl5XTu3LnZ9VvDmvI6Jr+5nUO7ZfD943PJiLlraV9o896Ubu0FtTldtKTNo0aNmu3uwyI3unuTL2BuxLo5TdT5OnBfwvLFwO8bKXt94jbgMOBvQOfw9SYwYk/fN3ToUG+JV155pUX1W2p7VY1/6Y5X/bhbZ/jardv3ynemus17W7q1111tThctaTPwtjdyXE12kDqqXFPdU6VAn4Tl3jR+aexYYErC8nnATHcvd/dy4O/AiUnG2ib99/SFfLC2jN9cMJiCrprCW0RSL9kE8baZ/dbMDjWzQ8zsDmB2E3VmAf3NrMjMcgiSwNMNC5nZEUAPgrOEeiuBL5pZlpllEwxQL0wy1jbnuffX8uCbK7j81CJGHaHHbIjIviHZBDEJqAL+CkwFtgPf3VMFd68BJgLPExzcp7r7fDO71czOTig6DigOT3XqTQM+At4D5gHz3P2ZJGNtU1Zv2c4Pps1jUO9ufP9LA1IdjojITslexVQB7HYfQxL1pgPTG6y7ucHy5Ih6tcAVn/f72pqa2jqumjKHOof/GTuEnCxN4S0i+46kjkhm9kJ45VL9cg8zez6+sNLDnS99yNsrNnPbeUdTmJ+X6nBERHaR7E/WfHffeZezu29Gz6RukX9+tIG7XlnC14f25pxjD051OCIiu0k2QdSZ2c6pNcyskIjZXSU5G8t3cHXxXIry8/jJOZrCW0T2TcnO5voj4HUzezVcHkF4g5p8Pu7OdY/OY8v2ah749nA65ST7n0BEZO9KdpD6OTMbRpAU5gJPEVzJJJ/Tn15fxiuL1nPrOUdx5EGawltE9l3JTtZ3GXAVwc1ucwluWnuTXR9BKk14r3Qrv3zuA848soCLT+yX6nBERPYo2TGIq4DjgRXuPopg1tX1sUXVDpXvqGHSlHfYr3MHfjVGU3iLyL4v2QRR6e6VAGbWwd0/AI6IL6z2xd256Yn3WLlpG78bO4TunXJSHZKISJOSHSEtDe+DeBJ4wcw2o0eOJu2xd1bz5Nw1XHvG4Qwv0hTeItI2JDtIfV74cbKZvQJ0A56LLap25KP15fz4yfc58ZCefHfUYakOR0QkaZ/7Gkt3f7XpUgJQWV3LpEfmkJudwe++MYTMDI07iEjboYvwY/SLv3/Ago8/5f7xwzigm6bwFpG2RbPDxeSFBZ/wwD+X851TijhtQEGqwxER+dyUIGLw8dbtfH/aPI4+uCvXf1kXe4lI26QE0cqCKbznUl1Tx+/HHUeHrMxUhyQi0iwag2hlv395CW8t38Qd3xhMkabwFpE2TGcQrWjm0o38/uUPOf+4gzlvSO9UhyMi0iJKEK1kc0UVVxfPpV+vPH56ztGpDkdEpMXUxdQK3J3vT5vHpooqHr/kZPI66M8qIm2fziBawQP/XM6LC9dx41cGcPTB3VIdjohIq1CCaKH3V2/l59M/4PSB+zP+5MJUhyMi0mqUIFogmMJ7Dj3zcvj1mMGawltE2hV1lrfAzU+9z4qNFTxy+Yn0yNMU3iLSvugMopkef6eUx99ZzaTT+nPiIb1SHY6ISKtTgmiGZRsquOnJ9xle1JNJp2kKbxFpn2JNEGZ2lpktMrMlZnZDxPY7zGxu+FpsZlsStvU1sxlmttDMFphZYZyxJmtHTS2TprxDTlYGd449lqxM5VgRaZ9iG4Mws0zgbuAMoBSYZWZPu/uC+jLufk1C+UkEz7qu9yBwm7u/YGadgbq4Yv08fvn3Rby/+lPu/dYwDuzWMdXhiIjEJs6fv8OBJe6+1N2rgGLgnD2UHwdMATCzI4Esd38BwN3L3X1bjLEm5aWFn3D/G8sYf3IhZxypKbxFpH0zd49nx2ZjgLPc/VPrwa0AAAs+SURBVLJw+WLgBHefGFG2HzAT6O3utWZ2LnAZUAUUAS8CN7h7bYN6E4AJAAUFBUOLi4ubHW95eTmdO3dudPvmyjp+/MZ2euZmcNOJueRktv1LWptqc3uTbu0FtTldtKTNo0aNmu3uw6K2xXmZa9QRtLFsNBaYlpAAsoBTCbqcVgJ/BcYDf9plZ+73APcADBs2zEeOHNnsYEtKSmisfm2dc9F9M6mzKv484Qscul/7+Me3pza3R+nWXlCb00VcbY6zi6kU6JOw3BtY00jZsYTdSwl154TdUzXAk8BxsUSZhLtfWcLMpZu49Zyj201yEBFpSpwJYhbQ38yKzCyHIAk83bCQmR0B9ADebFC3h5ntFy6fBixoWHdveGvZJn734mLOG3Iw/37cwakIQUQkJWJLEOEv/4nA88BCYKq7zzezW83s7ISi44BiTxgMCbuargNeMrP3CLqr7o0r1sZs2VbF1cVz6NuzEz8992hNpSEiaSXWqTbcfTowvcG6mxssT26k7gvAoNiCa4K784Np77K+fAeP/+cpdNYU3iKSZnSXVyMemrmCGQs+4fqzBnBMb03hLSLpRwkiwoI1n/Kzvy3ktAH7c+kXilIdjohISihBNLCtqoaJU96he8dsfj1mkMYdRCRtqWO9gVuems+yDRU8fNkJ9OrcIdXhiIikjM4gEjw1dzWPzi5l4qjDOPnQ/FSHIyKSUjqDCK3bVsetr7zPsH49uGp0/1SHIyKScjqDAP61dCM//1cl7s6d44ZoCm8REZQgmL1iMxfe9y8273CqautYu7Uy1SGJiOwT0j5BTH/vY2rrgpu46+qcmUs3pjgiEZF9Q9oniK8ccyA5mRlkANlZGXq+tIhIKO0TxNB+PZgy4UTO75/Nw5edyNB+PVIdkojIPkFXMREkibJDc5QcREQSpP0ZhIiIRFOCEBGRSEoQIiISSQlCREQiKUGIiEgkJQgREYmkBCEiIpGUIEREJJIShIiIRFKCEBGRSEoQIiISSQlCREQixZogzOwsM1tkZkvM7IaI7XeY2dzwtdjMtjTY3tXMVpvZXXHGKSIiu4ttNlczywTuBs4ASoFZZva0uy+oL+Pu1ySUnwQMabCbnwKvxhWjiIg0Ls4ziOHAEndf6u5VQDFwzh7KjwOm1C+Y2VCgAJgRY4wiItIIc/d4dmw2BjjL3S8Lly8GTnD3iRFl+wEzgd7uXmtmGcDLwMXAaGBYI/UmABMACgoKhhYXFzc73vLycjp37tzs+m1RurU53doLanO6aEmbR40aNdvdh0Vti/OBQRaxrrFsNBaY5u614fKVwHR3X2UWtZtwZ+73APcADBs2zEeOHNnsYEtKSmhJ/bYo3dqcbu0FtTldxNXmOBNEKdAnYbk3sKaRsmOB7yYsnwScamZXAp2BHDMrd/fdBrpFRCQecSaIWUB/MysCVhMkgQsbFjKzI4AewJv169z9ooTt4wm6mJQcRET2otgGqd29BpgIPA8sBKa6+3wzu9XMzk4oOg4o9rgGQ0REpFniPIPA3acD0xusu7nB8uQm9vEA8EArhyYiIk3QndQiIhJJCUJERCIpQYiISCQlCBERiaQEISIikZQgREQkkhKEiIhEUoIQEZFIShAiIhJJCUJERCIpQYiISCQlCBERiaQEISIikZQgREQkkhKEiIhEUoIQEZFIShAiIhJJCUJERCIpQYiISCQlCBERiaQEISIikZQgREQkkhKEiIhEUoIQEZFIShAiIhIp1gRhZmeZ2SIzW2JmN0Rsv8PM5oavxWa2JVx/rJm9aWbzzexdM/tGnHGKiMjusuLasZllAncDZwClwCwze9rdF9SXcfdrEspPAoaEi9uAb7n7h2Z2EDDbzJ539y1xxSsiIruK8wxiOLDE3Ze6exVQDJyzh/LjgCkA7r7Y3T8MP68B1gH7xRiriIg0ENsZBHAwsCphuRQ4IaqgmfUDioCXI7YNB3KAjyK2TQAmhIvlZraoBfHmAxtaUL8tSrc2p1t7QW1OFy1pc7/GNsSZICxinTdSdiwwzd1rd9mB2YHAQ8Al7l63287c7wHuaWmg4Xe97e7DWmNfbUW6tTnd2gtqc7qIq81xdjGVAn0SlnsDaxopO5awe6memXUF/gbc5O4zY4lQREQaFWeCmAX0N7MiM8shSAJPNyxkZkcAPYA3E9blAE8AD7r7ozHGKCIijYgtQbh7DTAReB5YCEx19/lmdquZnZ1QdBxQ7O6J3U8XACOA8QmXwR4bV6yhVumqamPSrc3p1l5Qm9NFLG22XY/LIiIiAd1JLSIikZQgREQkUtonCDO738zWmdn7qY5lbzCzPmb2ipktDKcyuSrVMcXNzHLN7C0zmxe2+SepjmlvMbNMM5tjZs+mOpa9wcyWm9l74bjl26mOZ28ws+5mNs3MPgj/vz6p1fad7mMQZjYCKCe4YuroVMcTt/DekgPd/R0z6wLMBs5NnAKlvTEzA/LcvdzMsoHXgavS4fJpM7sWGAZ0dfevpjqeuJnZcmCYu6fNjXJm9hfgH+5+X3gFaKfWmpYo7c8g3P01YFOq49hb3P1jd38n/FxGcIXZwamNKl4eKA8Xs8NXu/9lZGa9gX8D7kt1LBKP8H6xEcCfANy9qjXnrEv7BJHOzKyQYILEf6U2kviFXS1zCeb1esHd232bgd8BPwB2m4WgHXNghpnNDqfiae8OAdYDfw67Eu8zs7zW2rkSRJoys87AY8DV7v5pquOJm7vXuvuxBHf0Dzezdt2daGZfBda5++xUx7KXneLuxwFfBr4bdiG3Z1nAccAf3X0IUAHs9miF5lKCSENhP/xjwMPu/niq49mbwtPvEuCsFIcSt1OAs8M++WLgNDP7f6kNKX7h7M+4+zqC2RiGpzai2JUCpQlnxNMIEkarUIJIM+GA7Z+Ahe7+21THszeY2X5m1j383BE4HfggtVHFy91vdPfe7l5IMM3Ny+7+zRSHFSszywsvvCDsZjkTaNdXJ7r7WmBVOGURwGig1S44iXM21zbBzKYAI4F8MysFbnH3P6U2qlidAlwMvBf2yQP80N2npzCmuB0I/CV8iFUGwbQvaXHZZ5opAJ4IfgORBTzi7s+lNqS9YhLwcHgF01Lg262147S/zFVERKKpi0lERCIpQYiISCQlCBERiaQEISIikZQgREQkkhKEyOdkZpPN7LpUxyESNyUIkRQI78kQ2acpQYgkwcx+ZGaLzOxF4Ihw3aFm9lw4Mdw/zGxAwvqZZjYrfAZ7ebh+ZPgsjkeA98J13wyfVTHXzP6vPnGY2Zlm9qaZvWNmj4ZzZ4nsVUoQIk0ws6EE01UMAc4Hjg833QNMcvehwHXAH8L1dwJ3uvvxwJoGuxsO/MjdjzSzgcA3CCaYOxaoBS4ys3zgJuD0cOK5t4FrY2ugSCPSfqoNkSScCjzh7tsAzOxpIBc4GXg0nNoBoEP4fhJwbvj5EeA3Cft6y92XhZ9HA0OBWeE+OhJMR34icCTwRrg+B3iz1Vsl0gQlCJHkNJyTJgPYEv7y/zwqEj4b8Bd3vzGxgJl9jeCZFeM+f5girUddTCJNew04z8w6hrOFfg3YBiwzs69DMEuumQ0Oy88E/j38PHYP+30JGGNm+4f76Glm/cL6p5jZYeH6TmZ2eKu3SqQJShAiTQgf0fpXYC7BczT+EW66CLjUzOYB84FzwvVXA9ea2VsEM8lubWS/CwjGGmaY2bvACwTPC18PjAemhOtnAgNiaJrIHmk2V5FWZmadgO3u7mY2Fhjn7uc0VU9kX6MxCJHWNxS4K3w40xbgOymOR6RZdAYhIiKRNAYhIiKRlCBERCSSEoSIiERSghARkUhKECIiEun/A6P8S9BJTpjUAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#load train data\n",
    "DATA_TRAIN_PATH = '../data/train.csv'\n",
    "y, tX, ids = load_csv_data(DATA_TRAIN_PATH)\n",
    "\n",
    "# Data processing\n",
    "    #nanmed -> replace -999. with mean\n",
    "    #bound -> eliminate outliers\n",
    "    #zerovar -> eliminate columns with no varinaces\n",
    "My_options = ['nanmed', 'bound', 'zerovar']\n",
    "\n",
    "\n",
    "#Split (split=True) or cross (split=False)\n",
    "split = True\n",
    "ratio = 0.8\n",
    "seed = 1\n",
    "\n",
    "\n",
    "# Parameters\n",
    "# Every parameter needs to be in a list (if np.array write inside brackets por favor)\n",
    "degrees = [np.arange(1,7,1)]\n",
    "gammas = []\n",
    "lambdas = []\n",
    "method = least_squares\n",
    "\n",
    "# Gradient parameters, indicate if you are using a gradient method (True/False)\n",
    "Grad_method = False\n",
    "max_iter = 100\n",
    "w_init = []\n",
    "\n",
    "'''Everything above can be modified'''\n",
    "#************************************************************************************************\n",
    "'''Everything under should not be modified'''\n",
    "\n",
    "#pre-processing, spliting datas\n",
    "y, tX = process_data(y, tX, My_options)\n",
    "y_tr, x_tr, y_te, x_te = split_data(y, tX, ratio, seed)\n",
    "grad = [Grad_method, w_init, max_iter]\n",
    "\n",
    "# creation of weights and scores\n",
    "if split:\n",
    "    weights, losses = optimization(y_tr, x_tr, method, degrees, gammas, lambdas, grad)\n",
    "else:\n",
    "    weights, losses = optimization_cross(y, tX, method, degrees, gammas, lambdas, grad)\n",
    "\n",
    "    \n",
    "\n",
    "# few Plots as example\n",
    "\n",
    "logistic = False \n",
    "\n",
    "if 'prb' in My_options:\n",
    "    logistic = True\n",
    "    \n",
    "if len(weights) == 1:\n",
    "    if len(degrees[0]) == 1:\n",
    "        x_te = build_poly(x_te, degrees[0][0])\n",
    "    test_score(y_te, x_te, weights[0])\n",
    "elif lambdas :\n",
    "    if len(lambdas[0]) == 1:\n",
    "        plot_my_values(weights, y_te, x_te, degrees, gammas, [], logistic)\n",
    "else :\n",
    "    plot_my_values(weights, y_te, x_te, degrees, gammas, lambdas, logistic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test all the basic implementations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded\n",
      "least square GD learning ongoing...\n",
      "Score: % 70.748\n",
      "least square SGD learning ongoing...\n",
      "Score: % 69.452\n",
      "least square learning ongoing...\n",
      "Score: % 74.688\n",
      "ridge learning ongoing...\n",
      "Score: % 74.688\n",
      "logistic regression learning ongoing...\n",
      "Score: % 75.1365\n",
      "reg logistic regression learning ongoing...\n",
      "Score: % 75.09\n",
      "Best method is  logistic_regression : score =  0.751365\n"
     ]
    }
   ],
   "source": [
    "def test_main():\n",
    "    \"\"\"\n",
    "    TEST ALL RAW METHODS with optimal parameters found so far;\n",
    "    least_square_GD\n",
    "        parameters :\n",
    "            gamma = 7*10e-4 \n",
    "            max_iters = 1000\n",
    "        pre-processing :\n",
    "            nanmed\n",
    "            bound\n",
    "            std\n",
    "            \n",
    "    least_square_SGD\n",
    "        parameters : \n",
    "            gamma = 0.003\n",
    "            max_iters = 3000\n",
    "            batch_size = 1\n",
    "        pre-processing : \n",
    "            nanmed\n",
    "            bound\n",
    "            std\n",
    "            \n",
    "            \n",
    "    least_squares\n",
    "        parameters : \n",
    "            /\n",
    "        pre-processing :\n",
    "            nanmed\n",
    "            bound\n",
    "            \n",
    "    ridge_regression\n",
    "        parameters : \n",
    "            lambda_ = 10e-5\n",
    "        pre-processing :\n",
    "            nanmed\n",
    "            bound\n",
    "            \n",
    "    logistic_regression\n",
    "        parameters :\n",
    "            max_iters = 4000\n",
    "            gamma = 10e-7\n",
    "        pre_processsing :\n",
    "            nanmed\n",
    "            bound\n",
    "            std\n",
    "            prb\n",
    "            \n",
    "    reg_logisitc_regression\n",
    "       parameters :\n",
    "             max_iters = 2000\n",
    "            gamma = 10e-7 \n",
    "            lambda_ = 0.1\n",
    "        pre_processsing :\n",
    "            nanmed\n",
    "            bound\n",
    "            std\n",
    "            prb\n",
    "     \"\"\"\n",
    "\n",
    " \n",
    "    \n",
    "    # Load train and test data\n",
    "    DATA_TRAIN_PATH = \"../data/train.csv\"\n",
    "    y, tx, ids = load_csv_data(DATA_TRAIN_PATH)\n",
    "    \n",
    "    print('Data loaded')\n",
    "    \n",
    "    methods = ['least_square_GD', \n",
    "               'least_square_SGD', \n",
    "               'least_squares', \n",
    "               'ridge_regression', \n",
    "               'logistic_regression', \n",
    "               'reg_logistic_regression']\n",
    "    \n",
    "    scores = []\n",
    "    #for each method, store the accuracy score given by the test fucntion\n",
    "    for method in methods:\n",
    "        scores.append(test(y, tx, method))\n",
    "\n",
    "    \n",
    "    index = np.argmax(scores)   \n",
    "    print('Best method is ', methods[index], ': score = ', scores[index])\n",
    "    \n",
    "test_main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded\n",
      "Selection of jet 0\n",
      "Data processed\n",
      "best for degree  5  lambda: 1e-12 -> RMSE  =  0.680278061886013\n",
      "best for degree  6  lambda: 1e-12 -> RMSE  =  0.6793823605223961\n",
      "best for degree  7  lambda: 1e-12 -> RMSE  =  0.6782717805724462\n",
      "best for degree  8  lambda: 2.571913809059347e-12 -> RMSE  =  0.6780140644580992\n",
      "Selection of jet 1\n",
      "Data processed\n",
      "best for degree  5  lambda: 2.571913809059347e-12 -> RMSE  =  0.7571239649417154\n",
      "best for degree  6  lambda: 7.443803013251697e-10 -> RMSE  =  0.7517342195221864\n",
      "best for degree  7  lambda: 7.443803013251697e-10 -> RMSE  =  0.7507742210248669\n",
      "best for degree  8  lambda: 1e-12 -> RMSE  =  0.7496615590124668\n",
      "Selection of jet 2\n",
      "Data processed\n",
      "best for degree  5  lambda: 1.9144819761699535e-09 -> RMSE  =  0.7106451707154525\n",
      "best for degree  6  lambda: 1.1253355826007645e-10 -> RMSE  =  0.7057623927378783\n",
      "best for degree  7  lambda: 1e-12 -> RMSE  =  0.7045770688251414\n",
      "best for degree  8  lambda: 1e-12 -> RMSE  =  0.7032418141432465\n",
      "Selection of jet 3\n",
      "Data processed\n",
      "best for degree  5  lambda: 1.1937766417144357e-09 -> RMSE  =  0.7198628216101401\n",
      "best for degree  6  lambda: 1.7012542798525856e-11 -> RMSE  =  0.7065798410614118\n",
      "best for degree  7  lambda: 1e-12 -> RMSE  =  0.7052526102122008\n",
      "best for degree  8  lambda: 1e-12 -> RMSE  =  0.7008581897186767\n"
     ]
    }
   ],
   "source": [
    "# Load train data\n",
    "DATA_TRAIN_PATH = \"../data/train.csv\"\n",
    "y, tx, ids = load_csv_data(DATA_TRAIN_PATH)\n",
    "print('Data loaded')\n",
    "\n",
    "#creation of jet_num\n",
    "jets_y, jets_tX, _ = cat_variables(y, tx, ids)\n",
    "\n",
    "#iteration in jet_num\n",
    "for i in range(len(jets_y)):\n",
    "    \n",
    "    print('Selection of jet', i)\n",
    "    \n",
    "    #options of pre-processing\n",
    "        #nandel -> delete nan values (-999.)\n",
    "        #nanmed -> replace -999. with mean\n",
    "        #bound -> eliminate outliers\n",
    "        #std -> standardize data set\n",
    "        #prb -> change y into probability values\n",
    "        #zerovar -> eliminate columns with no variances\n",
    "    My_options = ['nanmed', 'bound', 'zerovar']\n",
    "    \n",
    "    #pre-processing\n",
    "    y, processed_tx_train = process_data(jets_y[i], jets_tX[i], My_options)\n",
    "    print('Data processed')\n",
    "\n",
    "    # Find the optimal parameters for ridge regression\n",
    "    optimal_degree, optimal_lambda = optimal_hyperparameters(y, processed_tx_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## accuracy test for the best model : ridge-regression with poly/cross-term/Jet values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jet number :  0\n",
      "validation accuracy :  0.8552269429014663\n",
      "jet number :  1\n",
      "validation accuracy :  0.8110129602166484\n",
      "jet number :  2\n",
      "validation accuracy :  0.8401151250496228\n",
      "jet number :  3\n",
      "validation accuracy :  0.8490863974734942\n",
      "total accuracy : 0.8379232415351693\n"
     ]
    }
   ],
   "source": [
    "def jet_accuracy_test(y, tX, methods, parameters):\n",
    "     \"\"\"\n",
    "    Method used to test the accuracy with the best model chosen,\n",
    "    from the train datas that we split into a set of train and a set of test data\n",
    "    Here we use the ridge_regression, with polynomial construction of parameters and cross-terms\n",
    "      parameters used for ridge-regression, i.e. lambdas and degres are taken from another method\n",
    "      that finds optimals parameters\n",
    "      \n",
    "    Arguments:\n",
    "        y: labels\n",
    "        tX: features matrix\n",
    "        methods: methhod used, here ridge-regression\n",
    "        parameters: additional parameters needed for the method, here lambdas\n",
    "        methods: method chosen for the best model\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    #ratio and seed for the spitting of datas\n",
    "    ratio = 0.8\n",
    "    seed = 1\n",
    "    \n",
    "    pred = []\n",
    "    Y =[]\n",
    "    \n",
    "    \n",
    "    #creation of jet_num\n",
    "    jets_y, jets_tX, _ = cat_variables(y, tX, ids)\n",
    "    \n",
    "    #options of pre-processing\n",
    "        #nandel -> delete nan values (-999.)\n",
    "        #nanmed -> replace -999. with mean\n",
    "        #bound -> eliminate outliers\n",
    "        #std -> standardize data set\n",
    "        #prb -> change y into probability values\n",
    "        #zerovar -> eliminate columns with no variances\n",
    "    My_options = ['nanmed', 'bound', 'zerovar','std']\n",
    "    \n",
    "    #iteration in each jet_num batch\n",
    "    for ind in range(len(jets_y)):\n",
    "        \n",
    "         #pre-process data\n",
    "        jets_y[ind], jets_tX[ind] = process_data(jets_y[ind], jets_tX[ind], My_options)\n",
    "        \n",
    "        #construction of the final features containing polynomiale features and cross-terms\n",
    "        final_tX = np.c_[build_poly(jets_tX[ind],degres[ind]), build_cross_terms(jets_tX[ind])]\n",
    "     \n",
    "        #splitting the data with a ratio of 0.8 in order to test the accuracy\n",
    "        y_tr, x_tr, y_te, x_te = split_data(jets_y[ind], final_tX, ratio, seed)\n",
    "       \n",
    "        #initial parameters y and tX + additional parameters here gammas\n",
    "        param = [y_tr, x_tr,gammas[ind]]\n",
    "\n",
    "        #construction of weights and losses\n",
    "        w,loss = test_methods(methods, param)\n",
    "        \n",
    "        #creation of prediction accuracy for each jt_num batch\n",
    "        pred_test = predict(x_te, w)\n",
    "        print(\"jet number : \", ind)\n",
    "        print(\"validation accuracy : \", np.sum(pred_test==y_te)/pred_test.shape[0])\n",
    "        \n",
    "        #lists of prediction and ids for each jet batch\n",
    "        pred.append(pred_test)\n",
    "        Y.append(y_te)\n",
    "    \n",
    "    # concatenation lists of labels and predictions for each jet batch in a single well arranged array\n",
    "    pred =np.concatenate(pred, 0)\n",
    "    Y = np.concatenate(Y,0)\n",
    "    \n",
    "    #final accuracy for the whole model\n",
    "    print(\"total accuracy :\", np.sum(pred==Y)/pred.shape[0])\n",
    "    \n",
    "#best parameters found\n",
    "gammas = [1e-12,7.443803013251697e-10,1.1253355826007645e-10,1.7012542798525856e-11]\n",
    "degres = [6,7,6,6]\n",
    "parameters = gammas\n",
    "\n",
    "#load the datas\n",
    "DATA_TRAIN_PATH = '../data/train.csv' # TODO: download train data and supply path here \n",
    "y, tX, ids = load_csv_data(DATA_TRAIN_PATH)\n",
    "jet_accuracy_test(y, tX, ridge_regression, parameters)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## submission of prediction with the best model : ridge-regresion with poly/cross-term/jet values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "Analyzing jet 0\n",
      "Analyzing jet 1\n",
      "Analyzing jet 2\n",
      "Analyzing jet 3\n",
      "Submission saved as  submissionfloflolog.csv\n"
     ]
    }
   ],
   "source": [
    "def main (methods):\n",
    "    \"\"\"\n",
    "      Main method, creating a submission .csv of predictions with the best model chosen\n",
    "      here we use the ridge_regression, with polynomiale construction of parameters and cross-terms\n",
    "      parameters used for ridge-regression, i.e. lambdas and degres are taken from another method\n",
    "      that finds optimals parameters\n",
    "    Arguments:       \n",
    "        methods: method chosen for the best model\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    #load datas \n",
    "    \"\"\"\n",
    "    y: labels for train\n",
    "    tx: features matrix for train\n",
    "    y_test: labels for test\n",
    "    tx_test: features matrix for test\n",
    "    \"\"\"\n",
    "    DATA_TRAIN_PATH = '../data/train.csv' \n",
    "    y, tX, ids = load_csv_data(DATA_TRAIN_PATH)\n",
    "\n",
    "    DATA_TEST_PATH = '../data/test.csv' \n",
    "    y_test, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)\n",
    "    \n",
    "    print(\"data loaded\")\n",
    "\n",
    "    #best parameters found from learning\n",
    "    gammas = [1e-12,7.443803013251697e-10,1.1253355826007645e-10,1.7012542798525856e-11]\n",
    "    degres = [6,7,6,6]\n",
    "    \n",
    "    \n",
    "    pred = []\n",
    "    id_t =[]\n",
    "    \n",
    "    #creation of jet_num\n",
    "    jets_y, jets_tX, _ = cat_variables(y, tX, ids)\n",
    "    jets_y_test, jets_tX_test, id_test = cat_variables(y_test, tX_test, ids_test)\n",
    "    \n",
    "    #options of pre-processing\n",
    "        #nandel -> delete nan values (-999.)\n",
    "        #nanmed -> replace -999. with mean\n",
    "        #bound -> eliminate outliers\n",
    "        #std -> standardize data set\n",
    "        #prb -> change y into probability values\n",
    "        #zerovar -> eliminate columns with no variances\n",
    "    My_options = ['nanmed', 'bound', 'zerovar','std']\n",
    "    \n",
    "    #iteration in each jet_num batch\n",
    "    for ind in range(len(jets_y)):\n",
    "        print('Analyzing jet {}'.format(ind))\n",
    "        \n",
    "        #pre-process data (train and test)\n",
    "        jets_y[ind], jets_tX[ind] = process_data(jets_y[ind], jets_tX[ind], My_options)\n",
    "        jets_y_test[ind], jets_tX_test[ind] = process_data(jets_y_test[ind], jets_tX_test[ind], My_options)\n",
    "\n",
    "        #construction of the final features containing polynomiale features and cross-terms\n",
    "        final_tX_train = np.c_[build_poly(jets_tX[ind],degres[ind]), build_cross_terms(jets_tX[ind])]\n",
    "        final_tX_test = np.c_[build_poly(jets_tX_test[ind],degres[ind]), build_cross_terms(jets_tX_test[ind])]\n",
    "\n",
    "        #parameters token by the chosen method\n",
    "        param = [jets_y[ind], final_tX_train, gammas[ind]]\n",
    "       \n",
    "        #create weights and losses\n",
    "        w,loss = test_methods(methods, param)\n",
    "        \n",
    "        #creation of predictions\n",
    "        pred_test = predict(final_tX_test,w)\n",
    "        \n",
    "        #lists of prediction and ids for each jet batch\n",
    "        pred.append(pred_test)\n",
    "        id_t.append(id_test[ind])\n",
    "\n",
    "\n",
    " \n",
    "    # concatenation lists of ids and predictions for each jet batch in a single well arranged array\n",
    "    pred =np.concatenate(pred, 0)\n",
    "    id_t =np.concatenate(id_t,0)\n",
    "\n",
    "\n",
    "    #submission\n",
    "    OUTPUT_PATH = \"../data/submissioncortana.csv\"    \n",
    "    create_csv_submission(id_t, pred, OUTPUT_PATH)\n",
    "    print('Submission saved as ', OUTPUT_PATH)\n",
    "\n",
    "\n",
    "main(ridge_regression)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
